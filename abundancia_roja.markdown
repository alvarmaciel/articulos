---
layout: post
title: "Plataformas para la abundancia roja"
author: "Nick Dyer-Witheford"
license: http://endefensadelsl.org/ppl_deed_es.html
---

Plataformas para la abundancia roja
===================================

> Publicado originalmente en Culture Machine Vol. 14 (2013) como _"Red
> Plenty Platforms"_.  Publicado bajo la Licencia de Producción de Pares
> con permiso del autor.

Introducción: Abundancia roja
-----------------------------

Shortly after the great Wall Street meltdown of 2008, a novel about
obscure and remote historical events provided an unexpected node for
discussion of the ongoing crisis. Francis Spufford’s Red Plenty (2010)
offered a fictionalized account of the failed attempt by Soviet
cyberneticians of the 1960s to establish a fully computerized system of
economic planning. Mixing historical figures – Leonid Kantorovich,
inventor of linear programming equations; Sergei Alexeievich Lebedev,
pioneering Soviet computer designer; Nikita Khrushchev, First Secretary
of the Communist Party – with imaginary ones, and setting them all in
motion through Kremlin corridors, rural collectives, industrial
factories and the Siberian science-city of Akademgorodok, Red Plenty
succeeded in the unlikely mission of making cybernetic planning a
page-turner. But the interest it attracted from economists, computer
scientists and political activists was not solely due to its narrative
of scientific endeavor and political intrigue; it also owed much to
timing.  Appearing amidst austerity and unemployment, as the world
market still teetered on the brink of collapse, Red Plenty could be
interpreted in different ways: a) as a cautionary tale that, recalling
Soviet debacles, reminds us capitalism remains the only game in town,
even if it has behaved badly (‘There Is No Alternative’); or b)
contrawise, as a recollection of unrealized potentialities, whispering
not just the quaint altermondialiste slogan, ‘another world is
possible’, but what David Harvey (2010: np) identifies as the more
cogent and subversive possibility, that of ‘another communism’.

Poco después de la gran caída de Wall Street del 2008, una novela acerca
de eventos históricos oscuros y remotos proveía un inesperado punto de
discusión sobre la crisis en marcha.  Abundancia roja, de Francis
Spufford (2010), ofrecía un recuento ficcionalizado del intento fallido
de los cibernéticos soviéticos de los '60 por establecer un sistema
completamente computarizado de planificación económica.  Mezclando
figuras históricas --Leonid Kantorovich, inventor de las ecuaciones de
programación lineal, Sergei Alexeievich Lebedev, pionero del diseño de
computadoras soviéticas, Nikita Khrushchev, Secretario General del
Partido Comunista-- con imaginarias y poniéndolas en acción en los
pasillos del Kremlin, colectivos rurales, fabricas industriales y la
ciudad científica siberiana de Akademgorodok, Abundancia roja tuvo éxito
en la improbable misión de convertir la planificación cibernética en un
libro atrapante.  Pero el interés que atrajo por parte de economistas,
informáticas y activistas políticas no fue solo por la narrativa
científica y la intriga política.  También le debió mucho al momento en
que se publico.  Al aparecer en el medio de la austeridad y el
desempleo, con el mercado global todavía al borde del colapso,
Abundancia roja puede interpretarse de distintas formas: a) como un
cuento con moraleja que al retrotraernos a las debacles soviéticas nos
recuerda que el capitalismo sigue existiendo, aun cuando no funcione del
todo bien ('no hay otra alternativa'); o b) contraintuitivamente, como
una recolección de potencialidades no realizadas, no solo susurrando el
pintoresco eslogan altermundialista 'otro mundo es posible', sino lo que
David Harvey [-@harvey-2010] identifica como la otra posibilidad, mas
fuerte y subversiva, la del 'otro comunismo'.

This paper takes Spufford’s novel as a starting point from which to
embark on an examination of the computing platforms that would be
necessary for a contemporary ‘red plenty’. It is not a discussion of the
merits and demerits of hacktivism, digital disobedience, electronic
fabrics of struggle, tweets in the street and Facebook revolutions, but
of digital communism. This is a topic that has already been touched on
by the wave of rethinking life after capitalism triggered by the 1989
implosion of the USSR, in proposals for ‘participatory economics’
(Albert & Hahnel, 1991), a ‘new socialism’ (Cockshott & Cottrell, 1993),
‘twenty first century socialism’ (Dieterich, 2006), or forms of
‘commonwealth’(Hardt & Negri, 2009). Unlike some of these sources,
however, this essay does not aim to provide detailed, often competitive,
‘blue-prints’ for a new society, but rather what Greig de Peuter, in a
personal conversation, once called ‘red-prints’- approximating
orientations to revolutionary possibilities.

Este articulo toma la novela de Spufford como el punto de partida desde
el que examinar las plataformas informáticas que serian necesarias para
una "abundancia roja" contemporánea.  No es una discusión sobre los
méritos o deméritos del hacktivismo, la desobediencia digital, los
entramados electrónicos de las luchas, twits en las calles o las
revoluciones por Facebook, sino del comunismo digital.  Este tema ya ha
sido tratado por una ola de repensadores de la vida luego del
capitalismo iniciada por la implosión de la URSS en 1989, en propuestas
como "economía participativa" [@albert-hahnel-1991], un "nuevo
socialismo" [@cockshott-cottrell-1993], "socialismo del siglo XXI"
[@dieterich-2006] o forma de "_commonwealth_" [@hardt-negri-2009].  Al
contrario de estas fuentes, este ensayo no intenta proveer cianotipos
detallados, a menudo competitivos, para una sociedad nueva, sino lo que
Greg de Peuter llamaba (en una conversación privada), "rojotipos", es
decir orientaciones aproximativas a posibilidades revolucionarias.


In discussing computing and communism it is almost impossible to escape
accusations of abandoning struggles and subjects to a machinic
determinism. Certainly all automatic, teleological, and evolutionary
models, including schematic choreographies of forces and relations of
production, should be rejected. Just as important, however, is the
avoidance of a contrary humanist determinism, which overstates the
autonomy and ontological privilege of ‘man versus machine’. Here, modes
of production, and the struggles that convulse them, are understood as
combinations of human and machine agents, entangled, hybridized and
co-determined DeleuzoDeLandian ‘assemblages’ (Thorburn, 2013).  That is
why the estimate sent to me by Benjamin Peters, historian of Soviet
cybernetics, that, compared with the machines available to the planners
of Red Plenty in, say, 1969, the processing power of the fastest
computer in 2019 will represent ‘roughly a 100,000,000,000 fold increase
in operations per second’, is exciting, a factoid that is, as Peters
remarks, ‘not itself meaningful but still suggestive’. The argument that
follows explores this suggestivity. This article thus looks at the most
direct through-line from Soviet cybernetics’ continuing attempts to
theorize forms of economic planning based on labour time algorithms and
super-computing. It then discusses how concerns about authoritarian
central planning might be affected by social media and software agents,
before going on to consider whether planning is redundant in a world of
automata, copying and replication. In partial answer to that last
question, ‘Red Plenty Platforms’ scans the role of cybernetics in the
planetary bio-crisis, concluding with some general observations about
cybernetics on today’s ‘communist horizon’ (Dean, 2012). First, however,
it reviews some of the problems, both practical and theoretical, that
were grappled with by the Soviet planners depicted in Red Plenty.

Al discutir informática y comunismo resulta casi imposible escapar a las
acusaciones de abandono de las luchas por un determinismo mecanicista.
Ciertamente todos los modelos automáticos, teleologicos y
evolucionistas, incluyendo las coreografías esquemáticas de fuerzas y
relaciones de producción, deben ser rechazados.  Resulta tan importante,
sin embargo, como evitar por el contrario un determinismo humanista, que
exagera la autonomía y el privilegio ontológico del "hombre contra la
maquina".  Aquí, los modos de producción y las luchas que los
convulsionan, son entendidos como combinaciones de agentes humanos y
mecánicos, enredados, híbridos y co-determinados "ensamblajes
deleuzo-delandianos" [@thorburn-2013].  Es por esto que la estimación
que me enviara Benjamin Peters, historiador de la cibernética soviética,
comparando las maquinas que los planificadores de Abundancia roja tenían
a disposición en, digamos 1969, con la computadora mas rápida de 2019
arroja que el poder de procesamiento de esta ultima representara
"aproximadamente un aumento de 100 mil millones de veces en operaciones
por segundo" resulta excitante, un hecho que es, como remarca Peters,
"no significativo en si mismo pero aun sugestivo".  El argumento que
sigue explora esta sugestividad.  Este articulo trata sobre la linea mas
directa en la continuidad de la cibernética soviética en cuanto a
teorización de formas de planificación económica basada en algoritmos de
tiempo de trabajo y supercomputación.  Ademas discute las preocupaciones
sobre el autoritarismo en la planificación centralizada y como es
afectado por los medios sociales y los agentes de software, antes de
pasar a considerar si la planificación se vuelve redundante en un mundo
de autómatas, junto con la copia y la replicación.  Como respuesta
parcial a la ultima pregunta, este articulo recorre el rol de la
cibernética dentro de la bio-crisis planetaria, concluyendo con algunas
observaciones generales sobre la cibernética en el "horizonte comunista"
actual [@dean-2012].  Primero, no obstante, revisa algunos de los
problemas, tanto prácticos como teóricos, con los que los planificadores
soviéticos de Abundancia roja se encontraron.


Is Capitalism a Computer?

¿El capitalismo es una computadora?
-----------------------------------

Digital philosophers suggest the universe may be a computer simulation
programmed by aliens: without engaging this position, there are grounds
for considering a more mid-range proposition, namely that capitalism is
  a computer. This is the contention implicit in one of the most serious
  intellectual challenges mounted against communist thought, ‘the
  socialist calculation problem’, formulated by ‘Austrian school’
  economists such as Ludwig von Mises (1935) and Frederick Hayek (1945).
  Writing in the period defined by the success of the Russian
  revolution, these economists attacked the premises and feasibility of
  the centrally planned economy. All social systems, they recognized,
  need some form of resource planning.  The market, however, enacts a
  distributed, spontaneous and emergent, non-coercive plan – what Hayek
  (1976: 38) called the ‘catallaxy’. Prices provide a synoptic,
  abstracted signal of heterogeneous and changing needs and conditions,
  to which entrepreneurial investment responds. A command economy, in
  contrast, must be both despotic and impractical, as calculating an
  optimal distribution of scarce resources depends on innumerable local
  knowledges about consumption needs and production conditions that no
  central reporting method could compile and evaluate.

Las filosofas digitales sugieren que el universo podría ser una
simulación por computadoras programada por extraterrestres.  Sin
involucrarse en esta posición, hay motivos para considerar una
proposición intermedia, es decir que el capitalismo es una computadora.
Esta es la contienda implícita en una de las mas serias respuestas
intelectuales al pensamiento comunista, "el problema del calculo
socialista", formulado por economistas de la escuela de Austria como
Ludwig von Mises [-@mises-1935] y Frederick Hayes [-@hayes-1945].
Escribiendo en el periodo definido por el éxito de la revolución rusa,
estos economistas atacaron las premisas y la factibilidad de la economía
centralmente planificada.  Todos los sistemas sociales, reconocían,
necesitan una forma de planificación de recursos.  El mercado, sin
embargo, funciona como un plan distribuido, espontaneo, emergente y
no-coercitivo --lo que Hayek llamo la "catalaxia" [-@hayek-1976].  Los
precios proveen una señal sinóptica y abstracta sobre condiciones y
necesidades cambiantes y heterogéneas a los que la inversión empresarial
responde.  Una economía comandada, en contraste, debe ser a la vez
despótica e impractica, porque el calculo de una distribución optima de
recursos escasos depende de innumerables conocimientos locales sobre las
necesidades de consumo y las condiciones de producción que ningún método
central de reporte podría compilar y evaluar.

The Austrian economists thus offered an update of Adam Smith’s
celebration of capital’s ‘invisible hand’, now re-envisioned as a
quasicybernetic information system: It is more than a metaphor to
describe the price system as a kind of machinery for registering change,
or a system of telecommunications which enables individual producers to
watch merely the movement of a few pointers as an engineer might watch
the hands of a few dials, in order to adjust their activities to changes
of which they may never know more than is reflected in the price
movement. (Hayek, 1945: 527)

Por lo tanto los economistas austriacos ofrecían una versión actualizada
de la "mano invisible" del capital de Adam Smith, ahora reconvertida en
un sistema de información cuasi cibernético:  es mas que metafórico
describir el sistema de precios como una especie de maquinaria para
registrar el cambio, o como un sistema de telecomunicaciones que permite
a los productores individuales observar algunos puntos como una
ingeniera observa las indicaciones de un medidor, para poder ajustar sus
actividades a cambios de los que no podrían saber mas que lo que se
refleja en el movimiento de precios [@hayek-1945].

Although he referred to telecommunications and engineering, Hayek,
writing in the final year of the Second World War, might as well have
invoked the giant mainframe computers of the Manhattan Project, for what
he proposed was that the market acted as an automatic calculating
engine: a computer.

Aunque se refería a las telecomunicaciones y la ingeniería durante el
ultimo año de la Segunda Guerra Mundial, Hayek bien podría haberse
referido a las gigantes _mainframes_ del Proyecto Manhattan, porque lo
que estaba proponiendo que el mercado actúa como un motor de calculo
automático: una computadora.

This was, however, a two-sided argument deployed polemically against
socialism. For if the market acts as a computer, why not replace it with
a computer? If central planning suffered from a calculation problem, why
not just solve it with real calculation machines? This was precisely the
point made by Hayek’s opponent, the economist Oskar Lange, who,
retrospectively reviewing the ‘socialist calculation’ debate, remarked:
‘today my task would be much simpler. My answer to Hayek … would be: so
what’s the trouble? Let us put the simultaneous equations on an
electronic computer and we shall obtain the solution in less than a
second’ (1967: 159). Such was the project of the cyberneticians featured
in Red Plenty, a project driven by the realization that the apparently
successful Soviet industrial economy, despite its triumphs in the 1940s
and ‘50s, was slowly stagnating amidst organizational incoherence and
informational bottlenecks.

Este es, sin embargo, un argumento de doble contra el socialismo.  Si el
mercado actúa como una computadora, ¿por qué no reemplazarlo por una?
Si la planificación centralizada sufría de un problema de calculo, ¿por
que no resolverla con maquinas de calculo reales?  Este fue precisamente
el argumento del oponente de Hayek, el economista Oskar Lange, que
refiriéndose en retrospectiva al debate sobre el "calculo socialista",
remarcaba: "hoy mi tarea hubiera sido mucho mas simple.  Mi respuesta a
Hayek hubiera sido: ¿cual es el problema?  Pongamos las ecuaciones
simultaneas en una computadora electrónica y obtendremos la solución en
menos de un segundo" [@lange-1967].  Este era el proyecto de las
cibernéticas de Abundancia roja, un proyecto motivado por la realización
de que la aparentemente exitosa economía industrial soviética, pese a
sus triunfos en los '40 y '50, se estaba estancando en medio de la
incoherencia organizativa y los cuellos de botella informaciones.

Their effort depended on a conceptual tool, the input-output table,
whose development is associated with two Russian mathematicians: the
émigré Wassily Leontief, who worked in the US, and the Soviet Union’s
Kantorovich, the central protagonist of Red Plenty. Inputoutput tables –
which, it was recently discovered, are amongst the intellectual
foundations of Google’s PageRank algorithm (Franceschet, 2010) – chart
the complex interdependence of a modern economy by showing how outputs
from one industry (e.g.  steel or cotton) provide inputs for another
(say, cars or clothing), so that one can estimate the change in demand
resulting from a change in production of final goods. By the 1960s such
tables were an accepted instrument of large scale industrial
organizations: Leontief’s work played a role in the logistics of the US
Air Force’s massive bomber offensive against Germany. However, the
complexity of an entire national economy was believed to preclude their
application at such a level.

Su esfuerzo dependió de una herramienta conceptual, la tabla de
entrada-salida, cuyo desarrollo está asociado a dos matematicos rusos:
el emigrado Wassily Leontief, que trabajo en EEUU y el sovietico
Kantorovich, protagonista de _Abundancia roja_.  Las tablas de
entrada-salida --que recientemente se han descubierto parte del
fundamento intelectual del algoritmo PageRank de Google
[@franceschet-2010]-- trazan la compleja interdependencia de una
economía moderna al mostrar como las salidas de una industria (por
ejemplo el acero o el algodón) proveen las entradas para otras
(automoviles o ropa), de forma que puede estimarse el cambio en la
demanda resultante de un cambio en la producción de bienes.  En los '60
estas tablas eran un instrumento aceptado por las organizaciones
industriales de gran escala: el trabajo de Leontief tuvo un rol en la
logistica del masivo bombardeo a Alemania por parte de las fuerzas
aereas estadounidenses.  No obstante, se creia que la complejidad de una
economía nacional completa impedia su aplicacion a tal nivel.


Soviet computer scientists set out to surmount this problem. As early as
the 1930s, Kantorovich had improved input-output tables with the
mathematical method of linear programming that estimated the best, or
‘optimizing’, combination of production techniques to meet a given
target. The cyberneticians of the 1960s aimed to implement this
breakthrough on a massive scale by establishing a modern computing
infrastructure to rapidly carry out the millions of calculations
required by Gosplan, the State Board for Planning that oversaw economic
five year plans. After a decade of experimentation, their attempt
collapsed, frustrated by the pitiful state of the Soviet computer
industry – which, being some two

Los cientificos informaticos sovieticos se propusieron resolver este
problema.  Ya en los '30, Kantorovich habia mejorada las tablas de
entrada-salida con el método matematica de programación lineal, que
estimaba la mejor, u "optimizaba", combinación de técnicas de producción
necesarias para un objetivo.  Los ciberneticos de los '60 intentaron
implementar ese descubrimiento a escala masiva, estableciendo una
infraestructura informatica moderna capaz de procesar los millones de
calculos requeridos por Gosplan, la Mesa Estatal de Planificación que
supervisaba los planes quinquenales económicos.  Luego de una decada de
experimentación, su intento colapsó, frustrado por el lamentable estado
de la industria informática soviética --que al estar dos decadas
atrasada con respecto a los EEUU, se perdió la revolución de la
computadora personal y no desarrollo un equivalente a Internet.  Por lo
tanto era totalmente inadecuado para lo que se proponia lograr.  Ademas
tenia la oposición de la _nomenklatura_, que veia en la planificación
informatica una amenaza a su poder burocratico y ayudo en el abandono
del proyecto [@castells-2000; @gerovitch-2008; peters-2012].

decades behind that of the US, missed the personal computer revolution
and did not develop an equivalent to the Internet. It was thus utterly
inadequate to the task set for it. All this, alongside political
opposition from a nomenklatura that, seeing in the new scientific
planning method a threat to its bureaucratic power, compelled
abandonment of the project (Castells, 2000; Gerovitch, 2008; Peters,
2012).  This was not the only twentieth century project of ‘cybernetic
revolutionaries’; as remarkable was the attempt by Salvador Allende’s
Chilean regime to introduce a more decentralized versión of electronic
planning, ‘Project Cybersyn’ (Medina, 2005). Led by the Canadian
cybernetician Stafford Beer, this was conceived as a system of
communication and control that would enable the socialist regime to
collect economic data, and relay it to government decision makers, even
while embedding within its technology safeguards against state
  micro-management and encouragement for many-sided discussions of
  planning decisions. This was an attempt at socio-technical engineering
  of democratic socialism that today perhaps seems more attractive than
  the post-Stalinist manoeuvres of the Soviet computer planners. But it
  met an even more brutal fate; Project Cybersyn was extinguished in the
  Pinochet coup of 1973.  In the end the failure of the USSR to adapt to
  a world of software and networks contributed to its economic/military
  defeat by the United States. Its disintegration, in which, as Alec
  Nove (1983) demonstrated, information bottlenecks and reporting
  falsifications played a major role, seemed to vindicate the Austrian
  economists.  Hayek’s praise of market catallaxy thus became central to
  the ‘neoliberal thought collective’ (Mirowski, 2009) that led the
  subsequent victory march of global capitalism.

Este no fue el unico proyecto de "cibernetica revolucionaria" del siglo
XXI.  Igual de remarcable fue el intento del gobierno de Salvador
Allende en Chile por introducir una versión decentralizada de
planificación electrónica, el "Proyecto Cybersyn" [@medina-2005].
Liderado por el cibernético canadiense Stafford Beer, fue concebido como
un sistema de comunicación y control que habilitaria al gobierno
socialista a recolectar información económica y presentarla a los
decisores politicos, aun cuando incluia en su tecnologia salvaguardas
contra la microgestión estatal y estimulos para discusiones
multilaterales sobre la planificación.  Este fue un intento de
ingeniería sociotecnica para el socialismo democrático que hoy en dia
parece mas atractivo que las maniobras post-estalinistas de los
planificadores sovieticos.  Pero se encontró con un destino mas brutal:
el Proyecto Cybersyn fue exterminado por el golpe pinochetista de 1973.
La falla de la URSS por adaptarse al mundo del software y las redes
contribuyó a su derrota economica y militar ante EEUU.  Su
desintegracion, en los que, como demostraba Alec Nove [-@nove-1983], los
cuellos de botella informaciones y la falsicación de reportes jugaron
un rol preponderante, pareció reinvindicar a los economistas austriacos.
Las alabanzas de Hayek a la catalaxia del mercado se volvieron centrales
al "pensamiento colectivo neoliberal" [@mirowski-2009] que lidero la
marcha victoriosa del capitalismo global.

The combined pressure of the practical disaster of the USSR and the
theoretical argument of the Austrian school exerted immense force inside
what remained of the left, pressuring it to reduce and reset the limit
of radical aspiration to, at most, an economy of collectively owned
enterprises coordinated by price signals. The many variants on such
‘market socialist’ proposals have evoked rebuttals from Marxists who
refuse to concede to commodity exchange. Perhaps because they grant to
the market the automatic informationprocessing functions ascribed by the
Austrian economists and market socialists, they may address issues of
technological innovation or public data availability, yet do not seem to
engage deeply with the potentialities of contemporary computing.

La presion combinada del desastre practico de la URSS y el argumento
teorico de la escuela de Austria ejerció una fuerza enorme dentro de lo
que quedaba de la izquierda, presionándola para reducir y redefinir el
limite de sus aspiraciones radicales a, como mucho, una economía de
empresas colectivamente apropiadas, coordinadas por señales de precios.
Las muchas variantes de tal "socialismo de mercado" han provocado el
rechazo de los marxistas que se resisten al intercambio de mercancias.
Tal vez porque le otorgan al mercado las mismas funciones de
procesamiento automático de información que los economistas austriacos y
los socialistas de mercado, pueden tocar temas como la innovación
tecnológica o la disponibilidad de datos publicos, pero no parecen
involucrarse profundamente con las potencialidades de la computación
moderna.

Today, post-crash, claims that markets are infallible information
machines may seem less credible than they did a quarter of century ago.
The parasitic energy-theft that underlies price-signal transmissions
(exploitation at the point of production); the inability of individual
commodity exchanges to register collective consequences (the so-called
‘externalities’); and the recursivity of a chrematistic system that
loops back on itself in financial speculation, have all become more
salient in the midst of global capital’s economic and ecological
implosion. But identifying such flaws does not excuse communists from
the requirement to specify how another system of resource allocation –
one avoiding the ‘serfdom’ of the statist subjugation Hayek (1944)
predicted – might work.

En la actualidad y despues de la crisis, decir que los mercados son
maquinas infalibles de información puede sonar menos creible que un
cuarto de siglo atras.  El parasitario robo energetico que subyace a las
transmisiones de señales de precios (es decir la explotacion en el
momento de la producción), la incapacidad de los intercambios
individuales de mercancias para registrar las consecuencias colectivas
(las llamadas "externalidades") y la recursividad de un sistema
crematistico que se vuelve sobre si mismo en la especulacion financiera
destacan cada vez mas en el medio de la implosion economica y ecologica
del capital global.  Pero la identificacion de estas fallas no excusa a
los comunistas de especificar como otro sistema de distribución de
recursos podría funcionar, sin caer en la "servidumbre" de la
subyugacion estatista que predijo Hayek [-@hayek-1994].

Labour Algorithms

## Algoritmos laborales

Despite the fall of actually-existing socialism, the idea of
computerized economic planning continued to be developed by small groups
of theorists, who have advanced its conceptual scope further than
anything attempted by Soviet cyberneticians. Two schools have been of
particular importance: the ‘New Socialism’ of Scottish computer
scientists Paul Cockshott and Alan Cottrell (1993); and the German
‘Bremen School’, which includes Peter Arno (2002) and Heinz Dieterich
(2006), the latter an advocate of Venezuelan-style ‘Twenty First Century
Socialism’. These tendencies have recently converged (Cockshott,
Cottrell & Dieterich, 2010). However, because little of the Bremen
group’s work is translated, the focus here will be on the New Socialism
of Cockshott and Cottrell.

A pesar de la caida del socialismo real, la idea de la planificación
central computarizada continuo siendo desarrollada por pequeños grupos
de teoricos, que han avanzado su alcance conceptual mas alla de lo que
habian intentando los ciberneticos sovieticos.  Dos escuelas han sido de
fundamental importancia:  el "Nuevo Socialismo" de los cientificos
informaticos escoseses Paul Cockshott y Alan Cottrell
[-@cockshott-cottrell] y la "Escuela de Bremen" alemana, incluyendo a
Peter Arno [-@arno-2002] y Heinz Dieterich [-@dieterich-2006], el ultimo
de los cuales es un militante del "Socialismo del Siglo XXI" al estilo
venezolano.  Estas tendencias han convergido recientemente
[@cockshott-cottrell-dieterich-2010].  Sin embargo, como muy pocas obras
de la Escuela de Bremen han sido traducidas, el foco aqui estara puesto
sobre el Nuevo Socialismo de Cockshott y Cottrell.

The distinguishing mark of the New Socialist project is its classic
Marxist rigor. Accordingly, its twenty-first century super-computer
planning follows to the letter the logic of the late nineteenth century
Critique of the Gotha Program (Marx, 1970), which famously suggests that
at the first, ‘lower’ stage to communism, before conditions of abundance
allow ‘to each according to his needs’, remuneration will be determined
by the hours of socially necessary labour required to produce goods and
services. In the capitalist workplace, workers are paid for the
reproduction of the capacity to labour, rather than for the labour
actually extracted from them; it is this that enables the capitalist to
secure surplus value.


La marca distintiva del proyecto del Nuevo Socialismo es el rigor
marxista clasico.  De esta forma, la planificación por supercomputadoras
del siglo XXI sigue al pie de la letra la logica de la Critica al
Programa de Gotha [@marx-1970] de finales del siglo XIX, que sugeria que
en el primer estadio del comunismo, antes que las condiciones de
abundancia permitan el "a cada cual segun su necesidad", la remuneracion
seria determinada por la cantidad de horas socialmente necesarias
requeridas para la producción de bienes y servicios.  En el espacio de
trabajo capitalista, los trabajadores son pagados por la reproduccion de
su capacidad de trabajo y no por el trabajo realmente extraido de ellos.
Esto es lo que permite al capitalismo asegurarse la plusvalia.


The elimination of this state of affairs, Cockshott and Cottrell
contend, requires nothing less than the abolition of money—that is,

La eliminación de este estado de hechos, dicen Cockshott y Cottrell,
requiere nada menos que la abolicion del dinero --es decir,

the elimination of the fungible general medium of exchange that, through
a series of metamorphoses of money in and out of the commodity form,
creates the self-expanding value that is capital. In their new
Socialism, work would be remunerated in labour certificates; an hour’s
work could be exchanged for goods taking, on a socially average basis,
an equivalent time to produce. The certificates would be extinguished in
this exchange; they would not circulate, and could not be used for
speculation. Because workers would be paid the full social value of
their labour, there would be no owner profits, and no capitalists to
direct resource allocation.  Workers would, however, be taxed to
establish a pool of labour-time resources available for social
investments made by planning boards whose mandate would be set by
democratic decisions on overall social goals.

la eliminacion del medio general de intercambio que, a traves de una
serie de metamorfosis desde y hacia la forma mercancia, crea el valor
auto-expandible que es el capital.  En su Nuevo Socialismo, el trabajo
seria remunerado en certificados de trabajo.  Una hora de trabajo podría
ser intercambiada por aquellos bienes que requieran la misma cantidad de
tiempo social promedio para ser producidos.  Los certificados quedarian
extintos en el acto, no son capaces de circular ni ser utilizados para
especular.  Como los trabajadores son retribuidos con el valor social
completo, no habria ganancias ni capitalistas para dirigir la
distribución de los recursos.  De todas formas los trabajadores pagarian
un impuesto que establezca un pozo de recursos en tiempo productivo,
disponible para las inversiones sociales hechas por mesas de
planificación cuyos mandatos serían establecidos por decisiones
democráticas sobre objetivos sociales generales.

Labour time thus provides the ‘objective unit of value’ for the New
Socialism (Cockshott & Cottrell 2003: 3). It is at this point that its
proponents invoke the capacities of information technology. Such a
system would require an enumeration of the labour time expended, both
directly and indirectly, in the creation of goods and services, to
assess the number certificates for which these goods and services can be
exchanged, and to enable the planning of their production. The basic
tool of the input-output table reappears, with special attention to
labour time, both as an input necessary for the production of goods, and
as an output that itself requires the inputs of training and education.
However, here the New Socialists have to confront a basic objection.
Since the fall of the USSR it has been conventionally accepted that the
scale of information processing attempted by its cyberneticians was
simply too large to be feasible. Writing in the 1980s, Nove (1983)
suggested that such an effort, involving the production of some twelve
million discrete items, would demand a complexity input-output
calculation impossible even with computers. This claim was repeated in
recent discussions of Red Plenty, with critics of central planning
suggesting that, even using a contemporary ‘desktop machine’, solving
the equations would take ‘roughly a thousand years’ (Shalizi, 2012).

El tiempo de trabajo provee "la unidad objetiva de valor" del Nuevo
Socialismo [@cockshot-cottrell-2003].  En este punto son invocadas las
capacidades de la tecnologia informatica.  Tal sistema requeriria la
enumeracion del tiempo de trabajo utilizado, tanto directa como
indirectamente, en la creacion de bienes y servicios, para evaluar la
cantidad de certificados necesarios y tambien para habilitar la
planificación economica.  La tabla de entrada-salida reaparece, poniendo
especial atencion en el tiempo de trabajo, tanto como una entrada
necesaria para la producción de bienes como una salida que requiere a su
vez las entradas del entrenamiento y la educacion.  No obstante, aqui
los Nuevos Socialistas deben confrontar una objecion básica.  Desde la
caida de la URSS se ha aceptado convencionalmente que la escala del
procesamiento de información que intentaron los ciberneticos sovieticos
fue simplemente demasiado grande.  En los '80, Nove [-nove-1983] sugeria
que tal esfuerzo, involucrando la producción de unos doce millones de
items discretos, demandaria una complejidad de calculos de
entrada-salida imposible aun con computadoras.  Esto fue repetido en las
discusiones recientes sobre _Abundancia roja_, donde los críticos de la
planificación central sugerian que aun con la "maquina de escritorio"
actual, resolver las ecuaciones tomaria "algo asi como mil años"
[@shalizi-2012].

Cockshott and Cottrell’s answer involves new tools, both conceptual and
technical. The theoretical advances are drawn from branches of computing
science that deal with abbreviating the number of discrete steps needed
to complete a calculation. Such analysis, they suggest, shows their
opponents’ objections are based on ‘pathologically inefficient’ methods
(Cockshott, in Shalizi, 2012).  The input-output structure of the
economy is, they point out,

La respuesta de Cockshott y Cottrell involucra más herramientas, tanto
conceptuales como tecnicas.  Los avances teoreticos son tomados de ramas
de la ciencia informatica que tratan con la abreviación de los pasos
discretos necesarios para completar una ecuacion.  Tal analisis,
sugieren, demuestra que las objeciones de los oponentes estan basadas en
metodos "patologicamente ineficientes" [@cockshott-2012].  La estructura
de entrada-salida de la economía es, dicen,

‘sparse’—that is to say, only a small fraction of the goods are directly
used to produce any other good. Not everything is an input for
everything else: yogurt is not used to produce steel. The majority of
the equations invoked to suggest insuperable complexity are therefore
gratuitous. An algorithm can be designed to short-cut through
input-output tables, ignoring blank entries, iteratively repeating the
process until it arrives at a result of an acceptable order of accuracy.

"dispersa" --es decir, solo una minima fracción de los bienes son
utilizados directamente para producir cualquier otro bien.  No todo es
una entrada para todo el resto:  el yogurt no es utilizado para producir
acero.  La mayor parte de las ecuaciones que se invocan para sugerir
complejidades insuperables son por lo tanto gratuitas.  Es posible
diseñar un algoritmo para encontrar atajos en las tablas de
entrada-salida, ignorando las entradas en blanco, repitiendo el proceso
iterativamente hasta que alcanza un resultado con un orden de precisión
aceptable.

The time would be further reduced by massive increases in computer
processing speed yielded by Moore’s Law. Suggesting high-level economic
planning is done on a ‘desktop machine’ is disingenuous. The issue is
supercomputing capacity. According to an email communication from
Benjamin Peters, in 1969, the time of Red Plenty, the ‘undisputed
workhorse’ of the Soviet information economy was the BESM-6 (‘bol’shaya
electronicheskaya schetnaya mashina’ – literally the ‘large/major
electronic calculating machine’), which could perform at an operating
speed of 800,000 flops or ‘floating operations per second’ – that is, at
8 megaflops, or 10^6 flops. By 2013, however, supercomputers used in
climate modelling, material testing and astronomical calculations are
commonly exceeding 10 quadrillion flops or ten ‘petaflops’. The holder
of the crown at the time of writing is Cray’s Titan at the Oak Ridge
National Laboratory achieving some 17.6 petaflops (10^15) (Wikipedia,
2013). Supercomputers with an ‘exaflop’ capacity (10^18 flops) are
predicted from China by 2019 (Dorrier, 2012).  Thus, as Peters (2013)
says, ‘giving the Soviets a bit generously 10^7 flops in 1969, we can
find (10^18 - 10^7 = 10^11) . . . a 100,000,000,000 fold increase’ by
today.

El tiempo podría reducirse masivamente por la velocidad de procesamiento
computacional predicha por la Ley de Moore.  Sugerir que la
planificación economica de alto nivel se realice en una "maquina de
escritorio" resulta poco sincero.  De acuerdo con una comunicación
electrónica de Benjamin Peters, en 1969 (la epoca de _Abundancia Roja_)
el "caballo de tiro indisputable" de la economía informatica soviética
era la BESM-6 ("_bolshaya electronicheskaya schetnaya mashina_",
literalmente "gran maquina calculadora electrónica"), que podría operar
a una velocidad de 800.000 flops u "operaciones flotantes por segundo",
es decir, a 8 megaflops, o 10^6^ flops.  En 2013, no obstante, las
supercomputadoras utilizadas en el modelado climatico, testeo de
materiales y calculos astronomicos generalmente sobrepasan los 10
cuadrillones de flops o diez "teraflops".  La mayor al momento de
escribir este articulo es la Titan de Cray, en el _Oak Ridge National
Laboratory_, alcanzando unos 17,6 petaflops, es decir 10^15^ flops
[@wikipedia-2013].  Las computadoras con una capacidad de 1 "exaflop", o
10^18^ flops, han sido predichas para el 2019 en China [@dorrier-2012].
Por lo tanto, como dice Peters [-@peters-2013], "otorgandole a los
sovieticos unos generosos 10^7^ flops en 1969, podemos encontrar que
10^18^ - 10^7^ = 10^11^ ... es decir un incremento de 100.000.000.000 de
veces".

With these capacities, Cockshott and Cottrell’s suggestion that the
computer requirements for large scale economic planning could be handled
by facilities comparable to those now used for meteorological purposes,
seems at least plausible. The ‘calculation problem’, however, involves
not just data processing but the actual availability of data; Hayek’s
claim was not merely that central planners cannot crunch economic
numbers fast enough, but that the numbers in a sense do not exist prior
to price setting, which provide an otherwise absent measure of
production performance and consumption activity. Again, Cockshott and
Cottrell suggest the answer lies in computers being used as a means of
harvesting economic information. Writing in the early 1990s, and
invoking levels of network infrastructure available in Britain at the
time, they suggest a coordinating system consisting of few personal
computers

Con estas capacidades, se vuelve plausible la sugerencia de Cockshott y
Cottrell donde los requerimientos computacionales de la planificación
económica de gran escala pueden ser manejados por instalaciones
comparables a las de las estaciones metereológicas actuales.  El
"problema de cálculo", no obstante, no solo involucra el procesamiento
de los datos sino también su disponibilidad.  La crítica de Hayek no
pasa solo por la velocidad imposible con la que los planificadores
centrales deberían procesar las cifras económicas, sino que esos números
no existen hasta el momento de la fijación del precio, que provee una
medida de otra forma ausente de performance de la producción y actividad
del consumo.  De nuevo, Cockshott y Cottrell sugieren que la respuesta
está en el uso de computadoras para la recolección de información
económica.  Escribiendo en los '90 e invocando los niveles de
infraestructura de red disponibles en la Inglaterra del momento,
sugieren un sistema de coordinación compuesto por unas pocas
computadoras personales

in each production unit, using standard programming packages, would
process local production data and send it by ‘telex’ to a central
planning facility, which every twenty minutes or so would send out a
radio broadcast of adjusted statistical data to be input at local
levels.  This is a scenario too reminiscent of the ramshackle
techno-futurism of Terry Gilliam’s Brazil. To bring the New Socialists
up to date we should instead refer to Fredric Jameson’s iconoclastic
vision of WalMart as ‘the shape of a Utopian future looming through the
mist’ (2009: 423). His point is that, if one for a moment ignores the
gross exploitation of workers and suppliers, Wal-Mart is an entity whose
colossal organizational powers model the planned processes necessary to
raise global standards of living. And as Jameson recognizes, and other
authors document in detail (Lichtenstein, 2006), this power rests on
computers, networks and information. By the mid 2000s Wal-Mart’s
data-centers were actively tracking over 680 million distinct products
per week and over 20-million customer transactions per day, facilitated
by a computer system second in capacity only to that of the Pentagon.
Barcode scanners and point of sale computer systems identify each item

en cada unidad productiva, que usando paquetes de programación estándar
procesarían los datos de la producción local y los enviarían por "telex"
a una instalación de planificación central, que cada 20 minutos
respondería por señales de radio con los datos ajustados
estadísticamente, para ser reutilizados en el nivel local.  Este
escenario recuerda mucho al destartalado tecno-futurismo que nos muestra
Terry Gilliam en _Brazil_.  Para actualizar a los Nuevos Socialistas
deberíamos referirnos más bien a la iconoclasta visión de Fredric
Jameson sobre _Wal-Mart_ como "la forma del futuro utópico avecinándose
entre la niebla" [-@jameson-2009].  El punto es que si por un momento
ignoramos la explotación de trabajadoras y proveedoras, _Wal-Mart_ es
una entidad cuyos colosales poderes organizativos modelan los procesos
planificativos necesarios para elevar los estándares globales de vida.
Como Jameson reconoce y otros autores documentan en detalle
[@lichtenstein-2006], este poder descansa sobre las computadores, las
redes y la información.  Para mediados de los 2000, los centros de datos
de _Wal-Mart_ procesaban activamente más de 680 millones de productos
distintos por semana y más de 20 millones de transacciones de venta por
día, todo esto facilitado por un sistema computacional solo seguido en
capacidad por el del Pentágono.  Los escáneres de código de barras y las
computadoras en los puntos de venta identifican cada ítem


sold, and store this information. Satellite telecommunications link
directly from stores to the central computer system, and from that
system to the computers of suppliers, to allow automatic reordering. The
company’s early adoption of Universal Product Codes had led to a ‘higher
stage’ requirement for Radio Frequency Identification (RFID) tags in all
products to enable tracking of commodities, workers and consumers within
and beyond its global supply chain.  Wal-Mart is significant because it
stands ‘at the front-edge of a seismic shift in the corporate
imaginary’. It is a shift that links the notion of a ‘logistics
revolution’ with ‘just-in-time-production’, and ‘harnesses emerging
digital and cybernetic technologies for managing production,
distribution and sales in as swift and efficient a manner as possible’
(Haiven & Stonemouth, 2009: np). This shift is spurred by the emergence
of an ‘Internet of Things’, relating digital information to real world
physical items through a network of sensor-instrumented products, users
and locations. Enabled by the spread of sophisticated 4G Wireless
networks, data storage-ondemand services via the ‘cloud’ from firms like
Amazon, and, especially, by the latest internet protocol IPV6’s
enlargement in addressability, which provides unique digital identifiers
for a ‘truly humongous 340 billion billion billion billion’ items, such
  device to device communication by now probably exceed in data volume
  the person-to-person traffic of the Internet (Economist, 2012; np). As
  Benjamin Bratton (2013) observes, such addressability, combined with
  digital coding compressed to the sub-microscopic level, opens up a
  virtually limitless capacity for the identification of not just of
  things and people, but also of their most elementary components and
  their relationships.  Thus the trajectory of both information
  processing speeds and data gathering capacities points to the
  suppression of the ‘socialist calculation problem.’ However, to speak
  of planning in such panoptic contexts is to inevitably invoke fears of
  omniscient state control. The New Socialists come from a vanguard
  Marxist-Leninist lineage, with a self-avowed ‘Jacobin’ centralist
  perspective (Cockshott, Cottrell, & Dieterich, 2011). To consider how
  cybernetic planning might be developed in more transparent and
  participatory modes, we need to look to different communist
  traditions.

vendido y almacenan su información.  Los comunicaciones satelitales
vinculan a las tiendas con el sistema central y esta a su vez con las
computadoras de los proveedores, posibilitando el re-abastecimiento
automático.  La adopción temprana de los códigos universales de producto
llevaron a un "estadío más alto" requiriendo que todos los productos
lleven etiquetas de Identificación por Radio Frecuencia (RFID) para
permitir el seguimiento de mercancías, trabajadoras y consumidoras
dentro y más allá de la cadena de suministro global.

Communist Agents Historically, the anti-statist tendency in Marxism has
been largely carried in a very different ‘worker council’ tradition,
that, against the powers of party and state has insisted on the role of
workplace assemblies as the loci of decision-making, organization and
power.  In an essay antediluvian by digital standards, ‘Workers'
Councils and the Economics of a Self-Managed Society,’ written in 1957
but republished in 1972, immediately after the Soviet crushing of
Hungary’s Workers Councils, Cornelius Castoriadis noted the frequent
failure of this tradition to address the economic problems of a ‘totally
self-managed society.’ The question, he wrote, had to be situated
‘firmly in the era of the computer, of the knowledge explosion, of
wireless and television, of input-output matrices’, abandoning
‘socialist or anarchist utopias of earlier years’ because ‘the
technological infrastructures … are so immeasurably different as to make
comparisons rather meaningless’ (Castoriadis, 1972: np).  Like the
planners of Red Plenty, Castoriadis imagines an economic plan determined
with input-output tables and optimizing equations governing overall
resource allocation (e.g. the balance between investment and
consumption), but with implementation in the hands of local councils.
His crucial point, however, is that there should be several plans
available for collective selection. This would be the mission of ‘the
plan factory’, a ‘highly mechanized and automated specific enterprise’,
using ‘a computer’ whose ‘memory’ would ‘store the technical
coefficients and the initial productive capacity of each sector’
(Castoriadis, 1972: np). This central workshop would be supported by
others studying the regional implications of specific plans,
technological innovations, and algorithmic improvements. The ‘plan
factory’ would not determine what social targets should be adopted;
merely generate options, assess consequences, and, after a plan has been
democratically chosen, up-date and revise it as necessary. Castoriadis
would agree with Raymond Williams’s (1983) later observation that there
is nothing intrinsically authoritarian about planning, providing there
is always more than one plan.  This early concept of cybernetic
self-management is a precursor of a more recent envisioning of
post-capitalism, Michael Albert and Robin Hahnel’s (1991) ‘Participatory
Economics’ or ‘Parecon’. This too emerges from a ‘workers council’
tradition, though from an anarchist, rather than Marxist line of
thought. Their work is famous for its model of ‘decentralized
participatory planning’ (Albert, 2003: 122), alternative to both market
mechanisms and central planning.  Councils are, again, the basic
societal units for democratic decision, but in Parecon these include not
just worker but consumer councils, too. Resource allocation is
determined by these organizations’ bids for different levels of
production and consumption, which over a series of rounds of negotiation
are progressively reconciled by Iteration Facilitation Boards. At
successive stages of the planning process, worker and consumer councils
are encouraged by the IFBs to revise their proposals in knowledge of
each other’s inputs, until enough convergence is produced to put a few
possible plans to a vote.  Parecon has been the topic of considerable
controversy. One of the most frequent objections is that it exemplifies
the problem Oscar Wilde identified when he remarked that ‘socialism is a
good idea but it takes too many evenings’ – i.e. it seems to require
endless meetings. Hahnel (2008: np) suggests both that increased social
interactivity is a positive feature of Parecon, and that its complexity
would not necessarily be greater than that of many routine requirements
of capitalist everyday life – shopping, taxes, finances, etc. But it
does appear that conducting the tiered and iterative planning cycles
they imagine at a speed sufficient to get anything done, would demand a
very sophisticated network infrastructure and a high level of
technologically mediated participation: extensive data banks accessed by
councils and individuals subjects, electronic swipe cards for the
measurement of labour and consumption, off-the shelf software for
proposal preparations, and just-time-inventory systems for production
(Albert, 2003: 133).

In fact Parecon seems to call for a digital development that postdates
its proposal: social media. A society of participatory, informed,
democratic and timely collective planning would require fast, varied and
interactive communicative platforms where proposals could be circulated,
responded to, at length or briefly, trends identified, reputations
established, revisions and amendments generated, and so on. It would, in
short, demand that Facebook, Twitter, Tumblr, Flickrr and other Web 2.0
platforms not only themselves become operations self-managed by their
workers (including their unpaid prosumer contributors), but also become
fora for planning: Gosplan with ‘tweets’ and ‘likes’. We also have to
think of these organs transformed in directions pioneered by experiments
in alternative social networks, such as Diaspora, Crabgrass, Lorea,
freed of profit incentives and centralized control and taking more
‘distributed’ and ‘federated’ forms (Cabello et al., 2013; Sevignani,
2013), becoming, as Hu and Halpin (2013) propose, networks that in their
     very format prioritize group projects over individual identities,
     or as platforms of ‘collective individuation’; not, perhaps social
     media as much as ‘council media’.  Yet perhaps the idea of everyone
     watching mobile screens lest they miss, not a Facebook poke, but
     voting the seventh iteration of the participatory plan, duplicates
     unattractive features of everyday life in high-tech capitalism. So
     we might speculate further, and suggest that what decentralized
     collective planning really needs is not just council media but
     communist agents: communist software agents.  Software agents are
     complex programmed entities capable of acting ‘with a certain
     degree of autonomy… on behalf of a user (or another program)’
     (Wikipedia, 2013b: np). Such agents manifest ‘goaldirection,
     selection, prioritization and initiation of tasks’; they can
     activate themselves, assess and react to context, exhibit aspects
     of artificial intelligence, such as learning, and can communicate
     and cooperate with other agents (Wikipedia, 2013b: np).
     Commercially, software ‘bidding agents’ are able to consistently
     outperform human agents so that ‘Humans are on the verge of losing
     their status as the sole economic species on the planet’ (Kephart,
     2002: 7207). The ability of such entities to create ‘perfect
           competition’ in electronic markets makes them a favorite of
           Austrian School-influenced economists (Mirowski, 2002). As
           preprogrammed buyers and sellers capable of processing vast
           amounts of market data, software agents have transformed
           electronic commerce because of their ability to quickly
           search the Internet, identify best offers, aggregate this
           information for users, or, indeed, make purchases
           autonomously. However, the arena in which such agents truly
           excel is in the financial sector, where high frequency
           trading is entirely dependent on software ‘bots’ capable of
           responding to arbitrage possibilities in milliseconds.  One
           can’t help but ask, however, what if software agents could
           manifest a different politics? Noting that Multi-Agent System
           models can be thought of as a means to answer problems of
           resource allocation, Don Greenwood (2007: 8) has suggested
           they could be geared toward solving the ‘socialist
           calculation problem’.  As planning tools, Multi-Agent
           Systems, he notes, have the advantage over real markets that
           ‘the goals and constraints faced by agents can be
           pre-specified by the designer of the model’ (Greenwood, 2007:
           9). It is possible to design agents with macro-level
           objectives that involve more than just the maximization of
           individual self-interest; two ‘welfare’ principles that
           economists have experimented with incorporating are equality
           and environmental protection sustainability.  Perhaps, then,
           we should envisage the repeated decision-cycles of democratic
           planning as being, not just debated and deliberated in social
           media, but partially delegated to a series of communist
           software agents, who absorb the attentional demands of the
           process, running at the pace of high-speed trading
           algorithms, scuttling through data rich networks, making
           recommendations to human participants (‘if you liked the
           geo-engineering plus nanotechnology but no-nukes five year
           plan, you might like…’), communicating and cooperating with
           each other at a variety of levels, preprogrammed to specific
           thresholds and configurations of decision (‘keep CO2
           emissions below 300 parts a million, increase incomes of the
           lower quintile… and no rise in labour hours necessary for a
           cup of coffee’).  In the age of autonomous machines, this may
           be what a workers’ council would look like.

Automata, Copies and Replicators Yet, is planning necessary at all?
Centralized, neo-socialist planning schemes and decentralized, networked
councilist versions both see computers as calculative instruments, a
means to measure, particularly to measure work: their aim is to abolish
capitalist exploitation by returning to workers the full worth of their
labour time. There is, however, another line of communist futurism which
understands computers not so much as instruments of planning as machines
of abundance. There are, we might say, two ways to beat Hayek’s
capitalist catallaxy. One is to out-calculate it. The other is to
explode it: scarcity is replaced with plenitude, ending the need for
either prices or planning. For Marxists, ‘plenty’ yields the transition
from the ‘lower’ phase of communism, which still must grapple with
problems of scarcity, to the higher phase of ‘from each according to his
abilities, to each according to his needs’. A popular metaphor for the
technological conditions necessary for this latter moment is the Star
Trek ‘replicator’, which automatically, and with a limitless energy,
provides for human needs (Fraise, 2011). This essay is not going to
adjudicate what level of needs satisfaction should be considered
‘enough’, or what combination of growth and redistribution is adequate
to attain it: this surely would be the issue facing the collective
planners of the future. It will, however, identify three cybernetic
tendencies that point towards the ‘higher’ phase of communism:
automation, copying and peer-to-peer production.  Automation has been
the most central to the communist imagination. Its classic statement is
the now-famous ‘Fragment on Machines’ in Grundrisse, where, looking at
the industrial factory of his age, Marx (1973: 690-711) predicts
capital’s tendency to mechanize production will, by destroying the need
for waged labour, blow up the entire system. The founder of cybernetics,
  Norbert Weiner (1950), saw its main consequence to be the computerized
  elimination of jobs. This digital ‘end of work’ thesis has been
  developed very bluntly by thinkers such as Andre Gorz (1985) and
  Jeremy Rifkin (1995). Over the late twentieth century, however,
  capital has notably avoided this scenario. Far from totally automating
  work, it has both sought out global reservoirs of cheap labour, and
  followed a ‘march through the sectors’ that pushes a moving front of
  labour commodification through agriculture, industry and services.
  Since 2000, however, the automation debate has been renewed.
  Continuing reductions in computing costs, improvements in vision and
  touch technologies, the military investments of the 9/11 wars in
  drones and autonomous vehicles, and wage demands by workers in China,
  India and other sources of formerly cheap labour has spurred a ‘new
  wave of robots… far more adept than those now commonly used by
  automakers and other heavy manufacturers’, more flexible and easier to
  train, that are now replacing workers not just in manufacturing but in
  distribution, circulation and service processes such as warehousing,
  call centres and even elder care (Markoff, 2012: np). Erik
  Brynjolfsson and Andrew McAfee (2011: 9), economists at the
  Massachusetts Institute of Technology, have sounded an alarm that the
  ‘pace and scale of this encroachment into human skills’ is now
  reaching a new level with ‘profound economic implications.’ These
  concerns are being echoed by mainstream economists (Krugman, 2012).
  Within capital, automation threatens workers with unemployment or
  production speed-up. If, however, there were no dominant structural
  tendency for increases in productivity to lead to unemployment or
  greater output without reduction in labour time, automation could
  systematically yield to less time spent in formal workplaces. In a
  communist framework that protected access to the use value of goods
  and services, robotization creates the prospect of a passage from the
  realm of necessity to freedom. It reintroduces the goal – closed down
  both within the Stakhanovite Soviet experiment and in the wage-raising
  trades unionism of the West – of liberating time from work, with all
  this allows both in terms of human selfdevelopment and communal
  engagement.  Juliet Schor’s (1991) estimate, that if American workers
  had taken gains won from productivity increases since the 1950s, not
  in wages but in time off, they would by 2000 have been working a
  twenty hour week. It indicates the scale of possible change. Proposals
  for a ‘basic income’ have recently figured in left politics. There are
    certainly criticisms to be made of these insofar as they are
    advanced as a reformist strategy, with the risk of becoming merely a
    rationalized welfare provision supporting neoliberal precarity. But
    it would be hard to envision a meaningful communist future that did
    not institute such measures to acknowledge the reductions in
    socially necessary labour time made possible by advances in science
    and technology, destroying Hayek’s calculation problem by
    progressively subtracting from it the capitalist ur-commodity,
    labour power.  If robots undermine the centrality of the wage
    relation, the Internet presents a parallel possibility, priceless
    goods. Mainstream economists have long recognized the anomalous
    features of nonrivalrous informational goods, which can be endlessly
    copied at almost zero cost, all but instantaneously circulated, and
    shared without detracting from their use value. As intellectual and
    cultural production have become increasingly digitized, these
    tendencies to make the Internet ‘a place of plenty’ (Siefkes, 2012:
    np) have become increasingly problematic for the price system.
    Capital has struggled to maintain the commodity form in cyberspace,
    either by attempts to enforce intellectual property, or by treating
    informational flows as advertising accelerators for other
    commodities. Nonetheless, the drift to software decommodification
    has proven ineradicable, and been intensified by the capacities to
    conduct this circulation outside of centrally controlled servers,
    through peer-to-peer networks. Piracy, which now accounts for the
    majority of digital music, games, film and other software
    distributed in Asia, Africa, Latin America and Eastern Europe
    (Karaganis et al., 2011) is the clandestine and criminalized
    manifestation of this tendency; and the free and open source
    software movement its organized expression.  The latter has been the
    focus of interest on the libertarian left since the inauguration of
    the Free Software Foundation (by Richard Stallman in 1984), which
    released code under a General Public License (GPL), guaranteeing
    users the freedom to repurpose, study, customize, redistribute, and
    change it. As Jacob Rigi (2012) observes, the so-called ‘copyleft’
    clause in the GPL, which requires that any program using GPL code is
    itself issued under GPL, is a ‘dialectical negation’ of copyright,
    because it simultaneously preserves and abolishes property in
    software, formulating ‘an allinclusive global property right’. This
    development was elaborated by Linus Torvalds’ organization in the
    early 1990s of the online voluntary collective cooperative method
    for open-source software production. As Rigi (2012) says, the
      combination of GPL license and Linux-style open source collective
      programming ‘represents the gist of the P2P [peer-to-peer] mode of
      production’; he sees in this an instantiation of Marx’s ‘higher
      communism’, acknowledging the collective nature of scientific
      knowledge, and rejecting any scarcitybased demand for ‘equivalence
      between contribution to social production and share of social
      product’.  Open source software has attained considerable
      practical success (Weber, 2004), while P2P production has
      developed in various directions, with its political inflection
      ranging from libertarian capitalism, to liberal views of the new
      ‘wealth of networks’ (Benkler, 2006) as supplementary to and
      compatible with markets, to specifically communist versions, such
      as the Oekonux project (Meretz,
2012), with the ecumenical Foundation for P2P Alternatives (Bauwens,
2012) working across the entire spectrum.  However, even if one regards
      open source and P2P as a germinal of a new mode of production,
      difficulties in cultivating this seed have become apparent. One
      such difficulty is the relative ease with which capital has
      incorporated this seed as a contribution to downstream
      commodification processes: indeed, the whole tendency of Web 2.0
      could be said to be the containment of ‘new’ P2P production and
      circulation methods firmly within the shell of capitalist ‘old’
      commodity forms. The other issue has been what Graham Seaman
      (2002) terms the ‘washing machine problem’ – the gulf between
      virtual and material production, cornucopian software and
      industrial production, which seems to restrict P2P practices,
      however progressive, to a small subset of total economic activity.
      Over the last decade, however, this gap has been narrowed by the
      rapid development of forms of computer controlled microfabrication
      devices: additive 3D printing is the most famous, but there are a
      variety of others, including subtractive micro-mills and other
      miniaturized and digitized engineering devices that put industrial
      capacities within the grasp of ‘hack labs’, households and small
      communities. These have provided the basis for an emerging ‘maker’
      movement, which links these digital manufacturing units to the
      networked circulation of design, suggesting to some that the ‘P2P
      mode of production can be extended to most branches of material
      production’ (Rigi, 2012). These technologies are also associated
      with the proliferation of robots and small-scale automata; indeed,
      the holy grail of the ‘maker’ movement is the self-replicating
      replicator, the perfect von Neumann machine. Extrapolation from
      these tendencies places the ‘fabbers’ and ‘replicators’ of sci-fi
      imagination much closer to realization than seemed possible even
      quite recently.  Even the most market-oriented of ‘makers’ don’t
      hesitate to point out that such developments appear to return the
      means of production back to popular hands (Doctorow, 2009;
      Anderson, 2012). But as the example of open source suggests, there
      is no intrinsic communizing logic in the maker movement, which
      could as easily result in a proliferation of
      micro-entrepreneurship as in a micro-industrial commons. In his
      critique of liberal P2P enthusiasts, Tony Smith observes that full
      development of commons-based peer production is ‘incompatible with
      the property and production relations of capital’ (2012: 178); as
      long as these relations persist those involved in volunteer peer
      production will continue to be explicated in the wage work on
      which they depend, their creations will be appropriated by capital
      as ‘free gifts’, and the wider development of such projects
      starved of resources.  However, in a world where investments were
      determined without systemically favouring the commodification of
      knowledge, and without the possibility of combining common goods
      with proprietary knowledge, the ‘immense emancipatory promise’ of
      peer-to-peer production could be fulfilled (Smith, 2012: 179). As
      Smith remarks, capital contains within itself a tendency to
      develop technologies ‘that allow certain types of use-values to be
      distributed in unlimited numbers to individuals at marginal costs
      approaching zero’ (2006, 341): ‘In any form of socialism worthy of
      the name, the costs of the infrastructure and social labour
      required to produce products such as these would be socialized and
      the products would be directly distributed as free public goods to
      any and all who wanted them’. Although Smith is sceptical that
      this tendency could, ‘in the foreseeable future’ become prevalent
      throughout the economy, he concedes that if it did, the Soviet
      experience, ‘plagued by scarcity issues’, would be ‘completely
      irrelevant to the socialist project’ (2006: 241-2).

Anthropocene Knowledge Infrastructures An abundant communist society of
high automation, free software, and in-home replicators might, however,
as Fraise (2011) suggests, need planning more than ever – not to
overcome scarcity but to address the problems of plenty, which
perversely today threaten shortages of the very conditions for life
itself. Global climate change and a host of interlinked ecological
problems challenge all the positions we have discussed to this point.
Bio-crisis brings planning back on stage, or indeed calculation – but
calculation according to metrics measuring limits, thresholds and
gradients of the survival of species, human and otherwise. Discussing
the imperatives for such ecosocialist planning, Michael Lowy (2009)
points out how this would require a far more comprehensive social
steering than mere ‘workers control’, or even the negotiated
reconciliation of worker and consumer interests suggested by schemes
such as Parecon.  Rather, it implies a far-reaching remaking of the
economic systems, including the discontinuation of certain industries,
such as industrial fishing and destructive logging, the reshaping of
transportation methods, ‘a revolution in the energy-system’ and the
drive for a ‘solar communism’ (Lowy, 2009: np).  Such transformations
would involve cybernetics along two major axes, as both contributors to
the current bio-crisis and as potential means for its resolution. On the
first of these axes, the ecological costs of nominally ‘clean’ digital
technologies have become increasing apparent: the electrical energy
requirements of cloud computing data-centres; the demands of chip
manufacture for fresh water and minerals, the latter from large scale
extractive enterprises; and the resulting prodigious quantities of toxic
e-waste. Making every home a fab-lab mini-factory will only speed-up
planetary heat death. Contrary to all idealistic notions of virtual
worlds, cybernetics are themselves inextricably part of the very
industrial system whose operations have to be placed under scrutiny in a
new system of metabolic regulation that aims for both red and green
plenty.  However, cybernetic systems are also a potential part of any
resolution of the bio-crisis – or, indeed, of even fully recognizing it.
Paul Edward’s (2010) A Vast Machine analyzes the global system of
climatological measurement and projection – the apparatus of weather
stations, satellites, sensors, digitally archived records and massive
computer simulations, which, like the Internet itself, originated in US
Cold War planning – on which comprehension of global warming rests. This
infrastructure generates information so vast in quantity and from data
platforms so diverse in quality and form that it can be understood only
on the basis of computer analysis. Knowledge about climate change is
dependent on computer models: simulations of weather and climate;
reanalysis models, which recreate climate history from historical data;
and data models, combining and adjusting measurements from multiple
sources.  By revealing the contingency of conditions for species
survival, and the possibility for their anthropogenic change, such
‘knowledge infrastructures’ of people, artifacts, and institutions
(Edwards, 2010:
17) – not just for climate measurement, but also for the monitoring of
    ocean acidification, deforestation, species loss, fresh water
    availability – reveal the blind spot of Hayek’s catallaxy in which
    the very grounds for human existence figure as an arbitrary
    ‘externality’.  So-called ‘green capital’ attempts to subordinate
    such bio-data to price signals. It is easy to point to the fallacy
    of pricing non-linear and catastrophic events: what is the proper
    tag for the last tiger, or the carbon emission that triggers
    uncontrollable methane release?  But bio-data and bio-simulations
    also now have to be included in any concept of communist collective
    planning. Insofar as that project aims at a realm of freedom that
    escapes the necessity of toil, the common goods it creates will have
    to be generated with cleaner energy, and the free knowledge it
    circulates have metabolic regulation as a priority. Issues of the
    proper remuneration of labor time require integration into
    ecological calculations. No bio-deal that does not recognize the
    aspirations of millions of planetary proletarians to escape
    inequality and immiseration will succeed, yet labour metrics
    themselves need to be rethought as part of a broader calculation of
    the energy expenditures compatible with collective survival.

Conclusion: For K-ommunism?  Marx (1964), in his famous, or notorious,
comparison of the ‘worst of architects’ and the ‘best of bees’, saw the
former distinguished by an ability to ‘erect in imagination’ the
structure he will create.  Today, with our improved knowledge of bee
communities, this distinction reeks of anthropocentricism. Yet even
alongside bees, beavers and other primates, humans manifest a
hypertrophic planning capacity. The Soviet experience, of which the
cyberneticians featured in Red Plenty were part, was only a narrow,
historically specific and tragic instantiation of this capability, whose
authoritarianism occludes the most crucial point in the Marxist concept
of planning, namely that it is intended as a means of communal election
of which, of a variety of trajectories, collective human
‘species-becoming’ might follow (Dyer-Witheford, 2004).  A new
cybernetic communism, itself one of these options, would, we have seen,
involve some of the following elements: use of the most advanced
super-computing to algorithmically calculate labour time and resource
requirements, at global, regional and local levels, of multiple possible
paths of human development; selection from these paths by layered
democratic discussion conducted across assemblies that include
socialized digital networks and swarms of software agents; light-speed
updating and constant revision of the selected plans by streams of big
data from production and consumption sources; the passage of increasing
numbers of goods and services into the realm of the free or of direct
production as use values once automation, copy-left, peer-to-peer
commons and other forms of micro-replication take hold; the informing of
the entire process by parameters set from the simulations, sensors and
satellite systems measuring and monitoring the species metabolic
interchange with the planetary environment.  This would indeed be a
communism heir to Lenin’s ‘soviets plus electricity’, with its roots in
red futurism, constructivism, tektology and cybernetics, together with
the left-science fiction imaginaries of authors such as Iain M. Banks,
Ken McLeod and Chris Moriarty. It would be a social matrix encouraging
increasingly sophisticated forms of artificial intelligence as allies of
human emancipation. For those who fear the march of the machine it holds
only this comfort: whatever singularities might spring from its networks
would not be those of entities initially programmed for unconstrained
profit expansion and the military defense of property, but rather for
human welfare and ecological protection. Such a communism is consonant
with a left accelerationist politic that, in place of
anarchoprimitivisms, defensive localism and Fordist nostalgia, ‘pushes
towards a future that is more modern, an alternative modernity that
neoliberalism is inherently unable to generate’ (Williams & Srnicek,
2013). If it needs a name, one can take the K-prefix with which some
designate ‘Kybernetic’ endeavors, and call it ‘K-ommunism’. The
possibile space for such a communism now exists only between the
converging lines of civilizational collapse and capitalist
consolidation. In this narrowing corridor, it would arise not out of any
given, teleological logic, but piece by piece from countless societal
breakdowns and conflicts; a post-capitalist mode of production emerging
in a context of massive mid-twenty-first century crisis, assembling
itself from a hundred years of non-linear computerized communist history
to create the platforms of a future red plenty.

References Albert, M. (2003) Parecon: Life After Capitalism. New York:
Verso.  Albert, M. & Hahnel, R. (1991) Looking Forward: Participatory
Economics for the Twenty First Century. Boston: South End Press.
Anderson, C. (2012) Makers: The New Industrial Revolution.  Toronto:
Signal.  Bauwens, M. (2005) ‘The Political Economy of Peer Production’,
CTheory, January 12: http://www.ctheory.net/articles.aspx?id=499
Benkler, Y. (2006) The Wealth of Networks. New York: Yale University
Press.  Brynjolsson, E, & McAfee, A. (2011) Race Against the Machine.
Lexington, MA: Digital Frontier.  Cabello, F. et al. (2013) ‘Towards a
Free Federated Social Web: Lorea Takes the Networks’, in G. Lovink & M.
Rasch (eds), Unlike Us Reader: Social Media Monopolies and Their
Alternatives.  Amsterdam: Institute of Network Cultures.  Castells, M.
(2000) End of Millennium.  Oxford: Oxford University Press.
Castoriadis, C. (1972) ‘Workers' Councils and the Economics of a
Self-Managed Society’:
http://www.marxists.org/archive/castoriadis/1972/workerscouncils.htm
Cockshott, P. & Cottrell A. (1993) Towards a New Socialism.  London:
Spokesman Books.  Cockshott, P., & Zachariah, D. (2012) Arguments For
Socialism, June 2: www.lulu.com Cockshott, P., Cottrell, A., Dieterich,
H. (2010) ‘Transition to 21st Century Socialism in the European Union’:
http://reality.gn.apc.org/econ/Berlinpaper.pdf Dean, J. (2012) The
Communist Horizon. London: Verso.  Dieterich, H. (2006) Der Sozialismus
des 21. Jahrhunderts – Wirtschaft, Gesellschaft und Demokratie nach dem
globalen Kapitalismus. Berlin: Homilius.  Doctorow, C. (2009) Makers.
New York: Tor.  Dorrier, J. (2012) ‘The Race to a Billion Billion
Operations Per Second: An Exaflop by 2018?’, SingularityHUB, January 11:
http://singularityhub.com/2012/11/01/the-race-to-a-billionbillion-operations-per-second-an-exaflop-by-2018/
Dyer-Witheford, N. (2004) ‘1844/2004/2044: The Return of Species-Being’,
Historical Materialism. 13(4): 3-25.  Economist (2012). ‘Welcome to the
thingternet: Things, rather than people, are about to become the biggest
users of the internet.’ The Economist, November 21:
http://www.economist.com/news/21566428-things-rather-peopleare-about-become-biggest-users-internet-welcome.

Edwards, P. (2010) A Vast Machine: Computer Models, Climate Data, and
the Politics of Global Warming. Cambridge, MA: MIT Press.  Franceschet,
M. (2010) ‘PageRank: Standing on the Shoulders of Giants’, Cornell
University Library, February 15: http://arxiv.org/abs/1002.2858
Gerovitch, S. (2008) ‘InerNyet: Why the Soviet Union Did Not Build a
Nationwide Computer Network’, History and Technology 24 (4): 335-350.
Gorz, A. (1985) Paths to Paradise: On the Liberation from Work.  London:
Pluto Press.  Greenwood, D. (2007) ‘From Market to Non-Market: An
Autonomous Agent Approach to Central Planning’, Knowledge Engineering
Review 22 (4): 349-360.  Hahnel, R. (2008) ‘Robin Hahnel Answers Various
Criticisms of Participatory Economics’, ZNet, November 19:
http://www.zcommunications.org/robin-hahnel-answers-variouscriticisms-of-participatory-economics-by-robin-hahnel
Haiven, M. & Stoneman, S. (2009) ‘Wal-Mart: The Panopticon of Time’,
Globalization Working Papers, Institute on Globalization and the Human
Condition: McMaster University, April:
http://www.academia.edu/1474872/WalMart_The_panopticon_of_time Hardt, M.
& Negri, A. (2009) Commonwealth. Cambridge, MA: Harvard University
Press.  Harvey, D. (2010) ‘Organizing for the Anti-Capitalist
Transition: Talk Given at the World Social Forum 2010, Porto Alegre’,
Reading Marx's Capital with David Harvey,
http://davidharvey.org/2009/12/organizing-for-the-anti-capitalisttransition/
Hayek, F. (ed.) (1935) Collectivist Economic Planning. London:
Routledge.  Hayek, F. (1976) Law, Legislation and Liberty v. 2: The
Mirage of Social Justice. Chicago: University of Chicago Press.

Hayek, F. (1945) ‘The Use of Knowledge in Society’, American Economic
Review 35 (4): 519-530.  Hayek, F. (1944) The Road to Serfdom. Chicago:
University of Chicago.  Hui, Y. & Halpin, H. (2013) ‘Collective
Individuation: The Future of the Social Web’, in G. Lovink & M. Rasch
(eds), Unlike Us Reader: Social Media Monopolies and Their Alternatives.
Amsterdam: Institute of Network Cultures.  Jameson, F. (2009) Valences
of the Dialectic. London: Verso.  Karaganis, J. (ed.) (2011) Media
Piracy in Emerging Economies. New York: Social Science Research Council.
Kephart, J. (2002) ‘Software Agents and the Route to the Information
Economy’, Proceedings of the National Academy of Sciences of the United
States of America, vol. 99, no. Suppl 3, May 14:7207-7213.  Krugman, P.
(2012) ‘Robots and Robber Barons’, New York Times (December 6):
ttp://www.nytimes.com/2012/12/10/opinion/krugman-robotsand-robber-barons.html?_r=0
Lange, O. (1967) ‘The Computer and the Market’, in C. H.  Feinstein
(ed.), Socialism, Capitalism and Economic Growth: Essays Presented to
Maurice Dobb. Cambridge: Cambridge University Press.  Lichtenstein, N.
(ed.) (2006) Wal-Mart: The Face of Twenty-First Century Capitalism. New
York: New Press.  Lovink, G. & Rasch, M. (eds) (2013) Unlike Us Reader:
Social Media Monopolies and Their Alternatives. Amsterdam: Institute of
Network Cultures.  Lowy, M. (2006) ‘Ecosocialism and Democratic
Planning’, in L.  Panitch & C. Leys (eds), Socialist Register 2007.
London: Merlin.  Marx, K. (1964) Economic and Philosophic Manuscripts of
1844. New York: International Publishers.


Marx, K. (1970) Critique of the Gotha Program. Moscow: Progress
Publishers.  Marx, K. (1973) Grundrisse. Harmondsworth: Penguin.  Marx,
K. (1977) Capital Vol. 1. New York: Vintage Books.  Medina, E. (2011)
Cybernetic Revolutionaries: Technology and Politics in Allende's Chile.
Cambridge, MA: MIT Press.  Mirowski, P. (2002) Machine Dreams: Economics
Becomes a Cyborg Science. Cambridge: Cambridge University Press.
Mirowski, P. (ed.) (2009) The Road from Mont Pelerin: The Making of the
Neoliberal Thought Collective. Cambridge, MA: Harvard University Press.
Nove, A. (1983) The Economics of Feasible Socialism. London: Allen &
Unwin.  Peters, A. (2001) Computer Sozialismus: Gespräche mit Konrad
Zuse.  Berlin: Verlag.  Peters, B. (2012) ‘Normalizing Soviet
Cybernetics’, Information & Culture 47(2) 145-175.  Rifkin, J. (1995)
The End of Work. New York: Putnam.  Rigi, J. (2012) ‘Peer-to-Peer
Production as the Alternative to Capitalism: A New Communist Horizon’,
Journal of Peer Production 1:
http://peerproduction.net/issues/issue-1/invited-comments/anew-communist-horizon/
Schor, J. (1991) The Overworked American: The Unexpected Decline of
Leisure. New York: Basic Books.  Seaman, G. (2002) ‘The Two Economies or
Why the Washing Machine Question is the Wrong Question’:
http://second.oekonuxconference.org/documentation/texts/Seaman.html
Sevignani, S. (2013) ‘Facebook vs. Diaspora: A Critical Study’, in G.
Lovink & M. Rasch (eds), Unlike Us Reader: Social Media Monopolies and
Their Alternatives. Amsterdam: Institute of Network Cultures.

Shalizi, C. (2012) ‘In Soviet Union, Optimization Problem Solves You’,
Crooked Timber (May 30):
http://crookedtimber.org/2012/05/30/in-soviet-unionoptimization-problem-solves-you/
Siefkes, C. (2012) ‘Beyond Digital Plenty: Building Blocks for Physical
Peer Production’, Journal of Peer Production 1:
http://peerproduction.net/issues/issue-1/invitedcomments/beyond-digital-plenty/
Smith, T. (2012) ‘Is Socialism Relevant in the ‘Networked Information
Age’? A Critical Assessment of The Wealth of Networks’, in A. Anton & R.
Schmitt (eds), Taking Socialism Seriously. Lanham: Lexington.  Smith, T.
(2006) Globalisation: A Systematic Marxian Account.  Boston: Brill.
Stallman, R. (2004) Free Software, Free Society. Thissur, India:
Altermedia.  Thorburn, E. (2013) ‘Minoritarian Assemblages: Embodied and
Machinic Agencies in the New Cycles of Struggle’, Journal of
Communication and Critical/Cultural Studies, forthcoming.  von Mises, L.
(1935) ‘Calculation in the Socialist Commonwealth’, in F.A. Hayek (ed.),
Collectivist Economic Planning. London: Routledge.  Weber, S. (2004) The
Success of Open Source. Cambridge, MA: Harvard University Press.
Wiener, N. (1950) Human Use of Human Beings: Cybernetics and Society.
Boston: Houghton Mifflin.  Wikipedia. (2013a) ‘TOP 500’:
http://en.wikipedia.org/wiki/TOP500 Wikipedia. (2013b) ‘Software
agents’: http://en.wikipedia.org/wiki/Software_agent Williams, A. &
Srnicek, N. (2013) ‘#ACCELERATE MANIFESTO for an Accelerationist
Politics’ (14 May):

http://criticallegalthinking.com/2013/05/14/acceleratemanifesto-for-an-accelerationist-politics
/ Williams, R. (1983) Towards 2000. London: Chatto & Windus.
